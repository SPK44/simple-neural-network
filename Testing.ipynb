{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_parser import parse_csv, enum_classes, prep_kaggle, reg_data, apply_reg\n",
    "from nn_layers import input_layer, hidden_layer, output_layer\n",
    "from neural_network import nn, d_tanh, d_cross_entropy_loss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = [\"Blues\",\"Country\",\"Electronic\",\"Folk\",\"International\",\"Jazz\",\"Latin\",\"New_Age\",\"Pop_Rock\",\"Rap\",\"Reggae\",\"RnB\",\"Vocal\"]\n",
    "x = parse_csv('train.x.csv', 1, 27)\n",
    "y = parse_csv('train.y.csv',1,2,'S20').astype(str)\n",
    "z = parse_csv('test.x.csv',1,27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x,0,x)\n",
    "\n",
    "def d_relu(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = nn(reg_data(x),enum_classes(y,dic))\n",
    "network.append_hidden_layer(100)\n",
    "network.append_hidden_layer(70, relu, d_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for Fold 0 is 2.1495103937944453 :: Training Loss for Fold 0 is 2.0841452505120848\n",
      "Test Loss for Fold 1 is 2.125993336354705 :: Training Loss for Fold 1 is 2.1042646914047207\n",
      "Test Loss for Fold 2 is 2.256064279232286 :: Training Loss for Fold 2 is 2.07660389808726\n",
      "Test Loss for Fold 3 is 2.176134441753202 :: Training Loss for Fold 3 is 2.112496767696643\n",
      "Test Loss for Fold 4 is 2.211250077518111 :: Training Loss for Fold 4 is 2.0930666372386617\n",
      "Test Loss for Fold 5 is 2.3049488175414687 :: Training Loss for Fold 5 is 2.1568448701129364\n",
      "Test Loss for Fold 6 is 2.207865746180881 :: Training Loss for Fold 6 is 2.116284028530511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.2045381560535855"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.k_fold_cross_validation(100000, 0.03, 1e-7, k=7, batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.test(apply_reg(z))\n",
    "l = network.get_output().T\n",
    "#prep_kaggle(l,dic,\"out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for Fold 0 is 2.6363118153097336 :: Training Loss for Fold 0 is 2.6219950807598074\n",
      "Test Loss for Fold 1 is 2.632925321601947 :: Training Loss for Fold 1 is 2.621665524822852\n",
      "Test Loss for Fold 2 is 2.6057817735228057 :: Training Loss for Fold 2 is 2.6043564123722085\n",
      "Test Loss for Fold 3 is 2.6324393354332543 :: Training Loss for Fold 3 is 2.6418659973935736\n",
      "Test Loss for Fold 4 is 2.63802872834303 :: Training Loss for Fold 4 is 2.6290156650941388\n",
      "Test Loss for Fold 5 is 2.6938610588191194 :: Training Loss for Fold 5 is 2.6886629767412247\n",
      "Test Loss for Fold 6 is 2.6408214299686854 :: Training Loss for Fold 6 is 2.6127986914169337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.6400242089997965"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.k_fold_cross_validation(100000, 0.3, 1e-7, k=7, batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for Fold 0 is 1.9538792013349349 :: Training Loss for Fold 0 is 1.9339338911018622\n",
      "Test Loss for Fold 1 is 1.9792293432345693 :: Training Loss for Fold 1 is 1.922724651508064\n",
      "Test Loss for Fold 2 is 2.0220168085680355 :: Training Loss for Fold 2 is 1.909181735520329\n",
      "Test Loss for Fold 3 is 1.9933276811269178 :: Training Loss for Fold 3 is 1.9418586883537952\n",
      "Test Loss for Fold 4 is 1.9584216773144205 :: Training Loss for Fold 4 is 1.9452748258576331\n",
      "Test Loss for Fold 5 is 1.9684687974882167 :: Training Loss for Fold 5 is 1.926496564670039\n",
      "Test Loss for Fold 6 is 1.9844773123641255 :: Training Loss for Fold 6 is 1.9290593668354887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9799744030616029"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.k_fold_cross_validation(100000, 0.003, 1e-7, k=7, batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for Fold 0 is 1.9918144733511376 :: Training Loss for Fold 0 is 1.9579669714959254\n",
      "Test Loss for Fold 1 is 1.976115764784223 :: Training Loss for Fold 1 is 1.9617651973802244\n",
      "Test Loss for Fold 2 is 1.9517959757978198 :: Training Loss for Fold 2 is 1.9633569239893887\n",
      "Test Loss for Fold 3 is 1.9943781018546403 :: Training Loss for Fold 3 is 1.9692228454221876\n",
      "Test Loss for Fold 4 is 2.014448442478685 :: Training Loss for Fold 4 is 1.9520886403205164\n",
      "Test Loss for Fold 5 is 1.9786617378727123 :: Training Loss for Fold 5 is 1.957604871073228\n",
      "Test Loss for Fold 6 is 2.0142695144244693 :: Training Loss for Fold 6 is 1.9614644819749394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9887834300805269"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.k_fold_cross_validation(500000, 0.0003, 1e-7, k=7, batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for Fold 0 is 1.9500520280756544 :: Training Loss for Fold 0 is 1.9293701232628917\n",
      "Test Loss for Fold 1 is 1.943701088682746 :: Training Loss for Fold 1 is 1.9217699688257917\n",
      "Test Loss for Fold 2 is 2.009060696600057 :: Training Loss for Fold 2 is 1.9107336609397847\n",
      "Test Loss for Fold 3 is 2.00495758078609 :: Training Loss for Fold 3 is 1.9173234989138288\n",
      "Test Loss for Fold 4 is 1.9834538712414822 :: Training Loss for Fold 4 is 1.9085075229607105\n",
      "Test Loss for Fold 5 is 1.936267289000532 :: Training Loss for Fold 5 is 1.9036905704804972\n",
      "Test Loss for Fold 6 is 1.9883095523325418 :: Training Loss for Fold 6 is 1.9187500215973623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9736860152455864"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network2 = nn(reg_data(x),enum_classes(y,dic))\n",
    "network2.append_hidden_layer(100)\n",
    "network2.append_hidden_layer(100, relu, d_relu)\n",
    "network2.k_fold_cross_validation(100000, 0.003, 1e-7, k=7, batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for Fold 0 is 1.9675424383483888 :: Training Loss for Fold 0 is 1.948111282912468\n",
      "Test Loss for Fold 1 is 2.009537854115796 :: Training Loss for Fold 1 is 1.9441415074192026\n",
      "Test Loss for Fold 2 is 2.0353494556908474 :: Training Loss for Fold 2 is 1.952319356000519\n",
      "Test Loss for Fold 3 is 1.9663630123726412 :: Training Loss for Fold 3 is 1.9442629084502618\n",
      "Test Loss for Fold 4 is 1.9976570751925542 :: Training Loss for Fold 4 is 1.9389259547315634\n",
      "Test Loss for Fold 5 is 1.9862021067803641 :: Training Loss for Fold 5 is 1.9482591030793477\n",
      "Test Loss for Fold 6 is 2.032188099647205 :: Training Loss for Fold 6 is 1.9497194188435267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9992628631639708"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network3 = nn(reg_data(x),enum_classes(y,dic))\n",
    "network3.append_hidden_layer(100)\n",
    "network3.append_hidden_layer(50, relu, d_relu)\n",
    "network3.k_fold_cross_validation(100000, 0.003, 1e-7, k=7, batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for Fold 0 is 1.980042048469743 :: Training Loss for Fold 0 is 1.935615200646746\n",
      "Test Loss for Fold 1 is 1.9804355450954056 :: Training Loss for Fold 1 is 1.9036250749836405\n",
      "Test Loss for Fold 2 is 1.9324310665522448 :: Training Loss for Fold 2 is 1.9086672352812128\n",
      "Test Loss for Fold 3 is 2.007666124266108 :: Training Loss for Fold 3 is 1.8938240590691693\n",
      "Test Loss for Fold 4 is 1.9174241566588812 :: Training Loss for Fold 4 is 1.899104093710573\n",
      "Test Loss for Fold 5 is 1.9929004705859226 :: Training Loss for Fold 5 is 1.8970896103638037\n",
      "Test Loss for Fold 6 is 1.9488635409956807 :: Training Loss for Fold 6 is 1.8964629199799123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9656804218034267"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network4 = nn(reg_data(x),enum_classes(y,dic))\n",
    "network4.append_hidden_layer(100)\n",
    "network4.append_hidden_layer(150, relu, d_relu)\n",
    "network4.k_fold_cross_validation(100000, 0.003, 1e-7, k=7, batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for Fold 0 is 1.941494117690159 :: Training Loss for Fold 0 is 1.885481100688127\n",
      "Test Loss for Fold 1 is 1.9453849848730291 :: Training Loss for Fold 1 is 1.8954864163587601\n",
      "Test Loss for Fold 2 is 1.9226151808267105 :: Training Loss for Fold 2 is 1.8831093031686568\n",
      "Test Loss for Fold 3 is 1.9777936469636415 :: Training Loss for Fold 3 is 1.9115919095205942\n",
      "Test Loss for Fold 4 is 2.0055612699527208 :: Training Loss for Fold 4 is 1.892415108580689\n",
      "Test Loss for Fold 5 is 2.006569353863405 :: Training Loss for Fold 5 is 1.8753509874206218\n",
      "Test Loss for Fold 6 is 1.935165419907447 :: Training Loss for Fold 6 is 1.9020769742924062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9620834248681589"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network4 = nn(reg_data(x),enum_classes(y,dic))\n",
    "network4.append_hidden_layer(100)\n",
    "network4.append_hidden_layer(175, relu, d_relu)\n",
    "network4.k_fold_cross_validation(100000, 0.003, 1e-7, k=7, batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for Fold 0 is 1.9520906910964126 :: Training Loss for Fold 0 is 1.8770957099357928\n",
      "Test Loss for Fold 1 is 1.9511004513430674 :: Training Loss for Fold 1 is 1.8775362021827848\n",
      "Test Loss for Fold 2 is 1.9490905584085654 :: Training Loss for Fold 2 is 1.906962944495741\n",
      "Test Loss for Fold 3 is 2.0231760834164385 :: Training Loss for Fold 3 is 1.8926711488890904\n",
      "Test Loss for Fold 4 is 2.006810026105528 :: Training Loss for Fold 4 is 1.8751837654542323\n",
      "Test Loss for Fold 5 is 1.9571009524468297 :: Training Loss for Fold 5 is 1.9052801880474663\n",
      "Test Loss for Fold 6 is 1.955528902704042 :: Training Loss for Fold 6 is 1.918652615650075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9706996665029837"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network5 = nn(reg_data(x),enum_classes(y,dic))\n",
    "network5.append_hidden_layer(120)\n",
    "network5.append_hidden_layer(175, relu, d_relu)\n",
    "network5.k_fold_cross_validation(100000, 0.003, 1e-7, k=7, batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_relu(x):\n",
    "    return np.maximum(x,0.01*x,x)\n",
    "\n",
    "def d_l_relu(x):\n",
    "    x[x<=0] = 0.01\n",
    "    x[x>0] = 1\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for Fold 0 is 1.989707149328143 :: Training Loss for Fold 0 is 1.8726180078127714\n",
      "Test Loss for Fold 1 is 2.0787279164795964 :: Training Loss for Fold 1 is 1.9052421642356758\n",
      "Test Loss for Fold 2 is 1.970843024553321 :: Training Loss for Fold 2 is 1.8774787020536015\n",
      "Test Loss for Fold 3 is 2.019736499680344 :: Training Loss for Fold 3 is 1.8395593183980583\n",
      "Test Loss for Fold 4 is 1.9919076476517918 :: Training Loss for Fold 4 is 1.8437671196259133\n",
      "Test Loss for Fold 5 is 2.024074472451978 :: Training Loss for Fold 5 is 1.8751668115870068\n",
      "Test Loss for Fold 6 is 1.9991063477780313 :: Training Loss for Fold 6 is 1.8943436062425225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0105861511318865"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network6 = nn(reg_data(x),enum_classes(y,dic))\n",
    "network6.append_hidden_layer(100, relu, d_relu)\n",
    "network6.append_hidden_layer(175, relu, d_relu)\n",
    "network6.k_fold_cross_validation(100000, 0.003, 1e-7, k=7, batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for Fold 0 is 1.995016549152023 :: Training Loss for Fold 0 is 1.9143418509998145\n",
      "Test Loss for Fold 1 is 1.9927297601272058 :: Training Loss for Fold 1 is 1.9092336066575775\n",
      "Test Loss for Fold 2 is 2.0189178027325987 :: Training Loss for Fold 2 is 1.9300824965547105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a68e2cffb794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnetwork7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_hidden_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnetwork7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_hidden_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_relu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnetwork7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_fold_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/neural_network.py\u001b[0m in \u001b[0;36mk_fold_cross_validation\u001b[0;34m(self, epochs, learning_rate, reg_lambd, k, batch)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/neural_network.py\u001b[0m in \u001b[0;36mset_mini_batch\u001b[0;34m(self, inD, outD, batch)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexamples\u001b[0m \u001b[0;31m#whole batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mex_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mbatchx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex_to_use\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mbatchy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex_to_use\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "network7 = nn(reg_data(x),enum_classes(y,dic))\n",
    "network7.append_hidden_layer(50)\n",
    "network7.append_hidden_layer(75, relu, d_relu)\n",
    "network7.k_fold_cross_validation(100000, 0.002, 1e-5, k=7, batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for epoch 0 is 2.564984263531225\n",
      "Loss for epoch 5000 is 2.4581823039906827\n",
      "Loss for epoch 10000 is 2.219287736423586\n",
      "Loss for epoch 15000 is 2.168805859740516\n",
      "Loss for epoch 20000 is 2.1150102738408223\n",
      "Loss for epoch 25000 is 2.0703982792946016\n",
      "Loss for epoch 30000 is 2.0322533219639487\n",
      "Loss for epoch 35000 is 1.9937050871710158\n",
      "Loss for epoch 40000 is 1.9639552581076076\n",
      "Loss for epoch 45000 is 1.9435549656832196\n",
      "Loss for epoch 50000 is 1.9283234026387897\n",
      "Loss for epoch 55000 is 1.9147805402753588\n",
      "Loss for epoch 60000 is 1.900147972652973\n",
      "Loss for epoch 65000 is 1.887309238411424\n",
      "Loss for epoch 70000 is 1.8771100509662824\n",
      "Loss for epoch 75000 is 1.866404376499664\n",
      "Loss for epoch 80000 is 1.858135337097643\n",
      "Loss for epoch 85000 is 1.847944325033084\n",
      "Loss for epoch 90000 is 1.8356679594368757\n",
      "Loss for epoch 95000 is 1.826390728950844\n"
     ]
    }
   ],
   "source": [
    "network8 = nn(reg_data(x),enum_classes(y,dic))\n",
    "network8.append_hidden_layer(100)\n",
    "network8.append_hidden_layer(175, relu, d_relu)\n",
    "network8.train(100000, 0.002, 1e-5, batch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for epoch 100000 is 1.8124370735542634\n",
      "Loss for epoch 105000 is 1.798711926878416\n",
      "Loss for epoch 110000 is 1.7861902003892163\n",
      "Loss for epoch 115000 is 1.7738496723804957\n",
      "Loss for epoch 120000 is 1.7565605463408498\n",
      "Loss for epoch 125000 is 1.7371808467797352\n",
      "Loss for epoch 130000 is 1.7244491944124107\n",
      "Loss for epoch 135000 is 1.7088488698665363\n",
      "Loss for epoch 140000 is 1.692864864409431\n",
      "Loss for epoch 145000 is 1.6729595549752203\n",
      "Loss for epoch 150000 is 1.6569110485288803\n",
      "Loss for epoch 155000 is 1.6393995587727246\n",
      "Loss for epoch 160000 is 1.6155029616748873\n",
      "Loss for epoch 165000 is 1.6010133575710923\n",
      "Loss for epoch 170000 is 1.5893824234319005\n",
      "Loss for epoch 175000 is 1.5740776449166802\n",
      "Loss for epoch 180000 is 1.5715409760592844\n",
      "Loss for epoch 185000 is 1.545444354643518\n",
      "Loss for epoch 190000 is 1.532958165492051\n",
      "Loss for epoch 195000 is 1.5309775793697558\n"
     ]
    }
   ],
   "source": [
    "network8.train(100000, 0.002, 1e-5, batch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for epoch 200000 is 1.4994915088317253\n",
      "Loss for epoch 200500 is 1.5290377794277632\n",
      "Loss for epoch 201000 is 1.516567429098232\n",
      "Loss for epoch 201500 is 1.5155927315692832\n",
      "Loss for epoch 202000 is 1.4883121015094263\n",
      "Loss for epoch 202500 is 1.5135844805194347\n",
      "Loss for epoch 203000 is 1.511435072124324\n",
      "Loss for epoch 203500 is 1.5231952976252763\n",
      "Loss for epoch 204000 is 1.5215681571007744\n",
      "Loss for epoch 204500 is 1.541208714159304\n",
      "Loss for epoch 205000 is 1.5306181747107004\n",
      "Loss for epoch 205500 is 1.5004865870361828\n",
      "Loss for epoch 206000 is 1.53126321176077\n",
      "Loss for epoch 206500 is 1.5461413345964914\n",
      "Loss for epoch 207000 is 1.503986358460306\n",
      "Loss for epoch 207500 is 1.5069516110893932\n",
      "Loss for epoch 208000 is 1.5363235211918536\n",
      "Loss for epoch 208500 is 1.5259609146381523\n",
      "Loss for epoch 209000 is 1.514470921194871\n",
      "Loss for epoch 209500 is 1.4970478479649023\n"
     ]
    }
   ],
   "source": [
    "network8.train(10000, 0.002, 1e-5, batch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for epoch 210000 is 1.5051028192750617\n",
      "Loss for epoch 210050 is 1.4456345421629122\n",
      "Loss for epoch 210100 is 1.4399675575299193\n",
      "Loss for epoch 210150 is 1.437230776754984\n",
      "Loss for epoch 210200 is 1.4354545210912237\n",
      "Loss for epoch 210250 is 1.4341293262678736\n",
      "Loss for epoch 210300 is 1.433068821888376\n",
      "Loss for epoch 210350 is 1.4321852688827041\n",
      "Loss for epoch 210400 is 1.431419949256544\n",
      "Loss for epoch 210450 is 1.4307453876043967\n",
      "Loss for epoch 210500 is 1.4301427198303454\n",
      "Loss for epoch 210550 is 1.4295944352182464\n",
      "Loss for epoch 210600 is 1.4290889116838645\n",
      "Loss for epoch 210650 is 1.4286203719941915\n",
      "Loss for epoch 210700 is 1.4281861190947331\n",
      "Loss for epoch 210750 is 1.427776008905116\n",
      "Loss for epoch 210800 is 1.427387286075301\n",
      "Loss for epoch 210850 is 1.427020479087119\n",
      "Loss for epoch 210900 is 1.42667020007802\n",
      "Loss for epoch 210950 is 1.426334302123942\n"
     ]
    }
   ],
   "source": [
    "network8.train(1000, 0.002, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "network8.test(apply_reg(z))\n",
    "l = network8.get_output().T\n",
    "prep_kaggle(l,dic,\"out2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
